\chapter{引言}
\par 目标物体的位姿估计问题是一个视觉感知问题，是机器人应用中重要的基础问题。只有机器人系统实现目标物体的定位，才能够实现环境交互，例如机械臂实现物体抓取、物体操作以及避障和跟踪等下游任务。目前，操作机器人还无法广泛应用于非工业场景，主要原因便是对于环境理解的能力不足，无法快速适应非结构化甚至未知环境。因此，解决位姿估计问题能够提高操作机器人的环境感知能力，提高操作机器人在非结构化环境中的快速部署能力。
\section{研究背景与意义}
\par 智能机器人是国家的重要战略产业，是未来人类社会发展的重要方向，具有重要的社会意义和经济价值。智能机器人的运行过程可以分为三个阶段：感知环境，规划行为以及执行操作。对环境的感知能力是限制智能机器人发展的一个主要原因。
\par 智能机器人、智能制造、智能物流等是国家重大需求，已经列入国家多个科技发展计划，具有重要社会意义和经济价值。机器人技术的发展推动社会的变革并带来行业的竞争，在我国《中国制造 2025》计划~\cite{中国制造2025} 和德国提出的工业 4.0~\cite{德国工业4.0}中，机器人科技作为“十大技术领域”被列出。
\par 继自动驾驶领域的快速发展，近两年来国内外具身智能公司发展迅猛，众多企业纷纷布局。从美国的波士顿动力（Boston Dynamics）公司的电驱Atlas机器人到特斯拉公司（Telsa）开发的擎天柱（Optimus）人型机器人再到李飞飞创办的空间智能公司World Labs，美国正走在具身智能的行业前沿。放眼国内，优比选、宇树、智元以及多个国家人形机器人创新中心俨然成为行业标杆，更有傅里叶智能、星海图、星尘智能等等数十家初创公司崭露头角。
\par 近两年来，大语言模型（Large Language Model）和基于图片-文本对训练的多模态模型在自然语言处理和计算机视觉领域取得了显著进展。例如OpenAI公司开发的GPT-4o模型，在语言理解、生成等多个领域表现出色，能够在多轮对话中完成上下文理解并输出高质量文本。
\par 本文以机器人对目标物体进行抓取作为研究背景，研究基于深度学习的目标物体的六自由度位姿估计（6-DoF Object Pose Estimation）方法，旨在提升机器人在各种环境下对刚性物体的视觉感知能力。
\par 目前机器人操作领域的研究百花齐放。OpenVLA~\cite{openvla}和RDT~\cite{liu2024rdt}等工作基于视觉-文本-机器人行为的多模态数据进行训练，实现了通过人类指令输入，机器人通过观测图像能够直接输出对应的动作。这类技术有极大的发展前景，但目前仍然受限于数据规模和数据质量，成功率有待提升，并且无法完成精细化的抓取操作。Unidexgrasp++~\cite{wan2023unidexgrasp++}和Yuyang Li等~\cite{li2024grasp} 采用强化学习路线学习机器人抓取动作，目前这类方法以成功抓取物体为目标，能够在仿真环境训练并在真实环境中部署，但无法准确控制目标物体的位姿。基于深度学习的位姿估计方法~\cite{hodan2024bop}利用大量的数据进行训练，能够在真实环境中准确估计目标物体的位姿，并且搭配轨迹规划算法，实现精细化的抓取操作。
\subsection{机器人抓取领域的应用}
\subsection{其他领域的应用}
\subsubsection{一个 Subsubsection}


\subsection{国内外研究现状}
\par 6自由度物体位姿估计问题是指通过传感器数据计算物体坐标系相对于相机坐标系的位姿变换。位姿变换包括平移分量和旋转分量，能用一个 $4\times 4$ 的位姿变换矩阵描述。传感器数据通常包括单目RGB相机、深度相机或者多目RGB相机或者多个视角的相机组合。物体坐标系和相机坐标系分别定义在物体的模型和相机上。物体的模型通常是一个三维模型，可以是点云、网格（mesh）模型或者CAD模型，通常具有纹理或颜色信息。相机的模型通常是一个消除了畸变的针孔相机模型，相机内参 $K$ 已知。
\par 刚性物体在空间中的位置和姿态具有六个自由度（6 Degree of Freedom，又称6DoF或6D），其中三个平移自由度，用笛卡尔坐标系中的坐标表示；三个旋转自由度，可以用欧拉角、四元数、旋转矩阵等方式进行表示。物体的6D位姿估计就是通过传感器获取场景图像、点云等信息，最终获得场景中目标物体在相机坐标系或者世界坐标系下的位置和姿态。
\par 传感器可以采用RGB彩色相机，RGB-D彩色深度相机或者双目立体相机，不同的传感器配置具有不同的特点。RGB彩色相机成本低，易于部署，但用于物体位姿估计的难度最大；RGB-D彩色深度相机的成本较高，且深度相机在帧率、视场角、深度测量范围的限制，使得深度相机不适用于高反光物体、透明物体或者快速运动的物体，但深度信息能够极大地提高位姿估计的精度，用于物体位姿估计的难度最低，目前大部分移动设备都未配备深度相机；双目立体相机能够用双目获取深度信息，但双目之间的距离限制了深度信息的精度，且目前数据集较少，相关研究也比较少。如何仅使用RGB彩色相机实现高精度的位姿估计，是目前的研究热点。

\par 物体位姿估计主要可以分为两类：实例级（instance-level）物体位姿估计和类别级（object level）物体位姿估计。实例级的物体位姿估计指的是物体的模型已知，即物体模型的长宽高均已知且准确。类别级的物体位姿估计的设定是给定物体的类别信息和一个模板化的模型，需要算法在输出位姿信息的同时输出物体模型的尺寸。举例来说，针对一个特定的杯子，实例级物体位姿估计方法需要给定该杯子的准确模型，而类别级物体位姿估计方法会估计这个杯子的长宽高三个方向的尺寸，由此定义物体的模型，进而进行位姿估计。这两类研究并行进展，总体上来说实例级物体位姿估计更能充分利用每一个模型的细节信息，因此能够得到更加准确的位姿估计，并且机器人操作往往需要定义抓取点，因此物体模型也是必要信息。类别级物体位姿估计有更好的泛化能力。本论文围绕实例级物体位姿估计算法展开研究，故以下相关研究仅围绕实例级物体位姿估计展开，不涉及类别级物体位姿估计的问题。
\section{基于RGB的物体位姿估计}
\subsection{传统方法}
\par 该部分传统方法指的是不使用网络的方法，只使用模式识别、特征提取等方式实现位姿估计的方法。传统方法可以分为迭代匹配（Iterative matching）~\cite{lowe1987three}，特征匹配（Feature matching）~\cite{rublee2011orb}，模板匹配（Template matching）~\cite{hinterstoisser2013model}等方法，如所示。迭代匹配方法通过迭代优化位姿，使相对应位姿渲染的边缘、轮廓等指定的特征与输入图片中的边缘、轮廓贴合。特征匹配方法通过寻找3维模型和2维图像中特征点的对应关系，然后使用perspective-n-point（PnP）算法~\cite{lepetit2009ep}计算得到物体的位姿。模板匹配方法将3维模型在各种姿态下渲染得到2维图像，通过查找与输入图像最相似的渲染得到的2维图像，将这张渲染得到的2维图像的位姿作为输入图像的位姿。对于无纹理物体（特征少）、前景遮挡（特征被遮挡）、背景杂乱（特征难提取）、环境光照变化（特征不鲁棒）、3维模型不准（特征无法匹配）等情况，都会导致传统方法结果不佳。
\subsection{深度学习方法}
\par 利用深度学习网络强大的拟合能力，将深度学习网络技术结合到位姿估计传统方法上的一些尝试，也取得了不错的效果。我们将基于网络的方法分为以下几类：直接回归法、稀疏特征匹配法、密集特征匹配法、模板匹配法、迭代优化法。
\subsubsection{直接回归法}
\par 直接回归法指的是直接从RGB图像作为输入，使用网络的方式对位姿进行分类或者回归的方法，该类方法的典型代表包括SSD-6D~\cite{kehl2017ssd}，Deep-6DPose~\cite{do2018deep}，PoseCNN~\cite{xiang2018posecnn}, EfficientPose~\cite{bukschat2020efficientpose}。
\subsubsection{稀疏特征匹配法}
\par 稀疏特征匹配法受到图像关键点检测算法的启发，首先利用网络的方式在二维平面上预测选取的关键点，然后利用关键点在平面上的坐标表示（2D）与关键点在三维模型中的坐标表示（3D）建立对应关系，该类方法的典型代表包括BB8~\cite{rad2017bb8}，YOLO6D~\cite{tekin2018real}，分割关键点法~\cite{hu2019segmentation}，DOPE。
\subsubsection{密集特征匹配法}
\par 密集特征匹配法不专门选取关键点，而是预测图像中物体区域的像素对应的三维坐标，利用稠密的2D-3D对应关系，计算物体的位姿。密集特征匹配法由于利用的信息更多，对于遮挡等情况有较好的鲁棒性。该类方法的典型代表包括iPose~\cite{hosseini2019ipose}，Pix2Pose~\cite{park2019pix2pose}，DPOD~\cite{zakharov2019dpod}，CDPN~\cite{li2019cdpn}等。
\subsubsection{模板匹配法}
\par 模板匹配法延续传统方法中模板匹配的思路，将三维模型在各种姿态下渲染得到二维图像，通过查找与输入图像最相似的渲染得到的三维图像，将这张渲染得到的二维图像的位姿作为输入图像的位姿。该方法的典型代表是AAE~\cite{sundermeyer2018implicit}。
\subsubsection{迭代优化法}
\par 迭代优化法延续传统方法中迭代匹配的思路，将渲染器得到的初始估计位姿渲染下的图像与输入图像进行对比，得出一个更加优化的估计位姿，重复上述过程最后得到更加精确的估计位姿。该类方法往往用于后处理，即提升其他方法的最终性能。该方法的典型代表包括DeepIM~\cite{li2018deepim}，CosyPose~\cite{labbe2020cosypose}等。
\section{基于RGB-D的物体位姿估计}
\subsection{传统方法}
\subsection{深度学习方法}
\section{面向泛化的物体位姿估计}
\section{数据集和评价指标}
\par Hinterstoisser等人~\cite{lm}于2013年发布的LM（LineMod）数据集最经典的数据集，其中的物体完全没有遮挡也没有光照变化，物体居于图片中央且易于区分，在位姿估计发展前期起到了重要的作用。LM-O（LineMod-Occlusion）数据集是Branchmann等人~\cite{lmo}在LM数据集的基础上标注了被遮挡物体的位姿，也成为了一个经典的数据集。Hodan等人\cite{tless}发布的T-LESS数据集是一个针对工业场景的数据集，包含多种无纹理对称性工业零件并且物体间存在强烈的遮挡关系。YCB-V（YCB-Video）是Yu Xiang等人~\cite{ycbv}发布的针对家庭场景的数据集，所有物体提供了具有纹理信息的模型，物体间存在遮挡关系，若干物体存在对称性。这四个数据集是物体位姿估计领域的经典数据集，被广泛应用于物体位姿估计的评估和比较。
% \par LM-O（LineMod-Occlusion）\cite{hinterstoisser2012model}是一个常用的物体位姿估计数据集，包含了15个物体的模型、渲染图像、真实图像、深度图像等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。
\par BOP（Benchmark for 6D Object Pose Estimation）\cite{hodan2018bop}将多个常用数据集整合成统一格式，并提供了统一的评价指标，成为了6D物体位姿估计领域的标准数据集。BOP数据集包括了多个数据集，如T-LESS、LM-O、ITODD、YCB-Video等，每个数据集包含了多个物体的模型、渲染图像、真实图像、深度图像等信息。
\section {研究内容与贡献}
\par 本文分别从RGB相机和RGB-D相机作为输入的两种情况，主要针对遮挡对实例级物体位姿估计的影响作为出发点，提出基于深度学习的物体位姿估计方法。对于RGB数据作为输入的情况，提出一种基于边缘的优化方法，进一步提升结果准确性。针对RGB-D数据作为输入的情况，研究RGB数据和深度信息数据进行更好的特征融合，提高位姿估计的召回率。最后，研究如何提高模型的泛化能力，使得模型在未见过的环境中也能够准确估计物体的位姿。
\section{本文组织架构}
\par 根据研究内容，本文共分为六个章节，各章节的内容安排如下：
\par 第一章绪论。
\par 第二章主要介绍了
\par 第三章主要介绍了
\par 第四章主要介绍了
\par 第五章主要介绍了
\par 第六章对本文的研究内容进行了总结，并对未来的研究方向进行了展望。