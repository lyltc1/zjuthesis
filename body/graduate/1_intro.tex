\chapter{绪论}

\section{研究背景与意义}
《中国制造2025》\cite{中国制造2025}战略系统阐述了机器人科技作为高端装备创新的重要载体，其产业布局涵盖汽车制造、精密机械、微电子、危险品制造、国防军工、化工及轻工业等领域工业机器人研发，并拓展至特种高危作业、医疗康复、居家养老、智慧教育等新型服务场景。德国工业4.0\cite{德国工业4.0}体系更将机器人技术列为“十大核心突破领域”，强调人机协作与智能柔性生产范式转变。作为先进制造与人工智能的融合载体，机器人革命正重构全球产业链价值分配图景，为社会经济结构转型注入核心驱动力。

近年来，全球机器人产业呈现爆发式增长态势\cite{2022中国智能机器人行业研究报告}。纵观全球产业格局，美国凭借波士顿动力公司（Boston Dynamics）的Atlas电驱仿人机器人\cite{bostondynamics}与特斯拉（Tesla）Optimus人形机器人\cite{optimus_wikipedia}等标杆性成果，持续引领具身智能技术革命。反观国内创新生态，宇树科技、优必选、智元机器人等头部企业，联合国家级人形机器人创新平台，已构筑起自主可控的产业技术生态。更值得关注的是，傅利叶智能、星海图、星尘智能等数十家科创企业相继推出差异化产品矩阵，在精密驱动、环境感知、智能决策等关键技术维度实现突破性迭代，标志着我国机器人产业正从"跟跑"向"并跑"战略转型。

智能机器人通过多模态感知系统已实现多元化作业场景的跨领域应用，涵盖工业物料转运\cite{bostondynamics_box}、精密装配操作\cite{stoegerautomation_spatz}及生活服务类任务执行\cite{xtech_robot}等典型范例。基于此，机器人需构建三维认知能力体系，具体包含环境感知、行为规划与操作执行三大核心要素\cite{蔡自兴2000机器人学}。尽管当前机电一体化技术已实现运动控制精度的显著突破\cite{doi:10.1177/02783649241285161}，但环境感知模块的智能化瓶颈仍是制约机器人自主化发展的关键掣肘\cite{doi:10.1126/scirobotics.adw1608}。

深度学习方法\cite{lecun2015deep}的突破性进展为机器人感知技术注入新动能，推动目标识别\cite{objectDetectionSurvey}、特征点定位\cite{keypointSurvey}及三维位姿估计\cite{poseEstimationSurvey}等关键技术迭代升级。特别值得关注的是，刚性物体六自由度位姿估计（6-DoF Object Pose Estimation）作为机器人视觉感知的核心技术，直接影响抓取操作的精准性与可靠性。本研究以机器人灵巧抓取为应用背景，聚焦基于深度学习的六自由度位姿估计方法创新，致力于构建具备环境鲁棒性的视觉感知框架，进而增强智能装备对复杂工况的适应能力。

\section{什么是位姿估计}

\autoref{fig:位姿估计示意图}展示了位姿估计任务的示意图。通俗而言，"图中蓝色水壶距相机前方约0.8米，其几何中心在图像坐标系中呈现左偏特性"，是对物体位置的描述；"观测视角呈俯视状态，壶口朝向右前方空间"，则是对物体姿态的描述。然而，这种基于常识的经验性表述缺乏精确性。本研究将采用数学化的表达方式，对物体的位姿进行精确建模与描述。

以图中的蓝色水壶为例，我们首先通过三维扫描或人工建模手段获取该物体的CAD模型，并在该模型上定义物体局部坐标系，例如将坐标原点设于模型几何中心。随后，以特定视角通过相机对物体进行拍摄，获得一张观测图像。通过对观测图像进行分析，我们能够估计出物体局部坐标系在相机坐标系中的表达，这一过程即为位姿估计。位姿估计的结果由平移向量$\mathbf{t}$和旋转矩阵$\mathbf{R}$两部分组成，可统一表示为刚性变换矩阵$\mathbf{T}=[\mathbf{R}|\mathbf{t}]$。

\begin{figure}[htbp]
    \centering
    \begin{overpic}[width=0.68\textwidth]{figure/intro/位姿估计示意图.jpg}
        \put(2,15){CAD模型}
        \put(10,51){相机坐标系}
        \put(32,25){观测图像}
        \put(83,39){物体坐标系}
        \put(67,52){$\mathbf{T}=[\mathbf{R}|\mathbf{t}]$}
        \put(5,30){$\mathbf{X}_C$}
        \put(14,26){$\mathbf{Y}_C$}
        \put(38,35){$\mathbf{Z}_C$}
        \put(78,13){$\mathbf{X}_O$}
        \put(78,25){$\mathbf{Y}_O$}
        \put(64,27){$\mathbf{Z}_O$}
    \end{overpic}
    \caption{位姿估计示意图}
    \label{fig:位姿估计示意图}
\end{figure}

在获得物体位姿变换矩阵$\mathbf{T}$的基础上，结合相机内参矩阵与畸变系数等标定参数，可基于三维变换矩阵的坐标映射关系，通过计算机图形学技术将CAD模型投影至二维成像平面。如\autoref{fig:渲染示意图}所示，经此刚性变换渲染生成的合成图像中，目标物体的几何投影位置与空间姿态均与原始观测图像实现亚像素级吻合，从而验证了位姿估计结果的数学正确性与物理一致性。

\begin{figure}[htbp]
    \centering
    \begin{overpic}[width=0.68\textwidth]{figure/intro/渲染示意图.jpg}
        \put(10,5){CAD模型}
        \put(33,32){渲染}
        \put(29.5,38){$\mathbf{T}=[\mathbf{R}|\mathbf{t}]$}
        \put(66.5,21){一致}
        \put(58,3){渲染图片}
        \put(97,3){拍摄图片}
    \end{overpic}
    \caption{位姿渲染图片示意图}
    \label{fig:渲染示意图}
\end{figure}

\section{研究脉络梳理}

本节系统阐述六自由度位姿估计技术的多维度分类体系，从核心方法演进、训练数据类型及传感器配置三个层面展开论述，并进一步探讨该技术在实际应用中面临的共性困难与挑战。通过构建层次化的分类框架与问题剖析，为后续研究综述提供分析基础。

\subsection{时间线}

从时间维度划分，位姿估计研究可划分为传统方法时代与深度学习方法时代两大阶段，如\autoref{fig:时间线}所示。

\begin{figure}[htbp]
    \centering
    \begin{overpic}[width=1.0\textwidth]{figure/intro/时间线.jpg}
        \put(7,18){1987}
        \put(6.5,14.5){线特征\cite{lowe1987three}}
        \put(16,18){2004}
        \put(17,21){点特征\cite{SIFT}}
        \put(26.5,18){2010}
        \put(25,14.5){点对特征\cite{PPF}}
        \put(42,18){2012}
        \put(42,14.5){模板匹配\cite{lm}}
        \put(60.5,18){2017}
        \put(61,21){PoseCNN\cite{xiang2018posecnn}}
        \put(70,18){2018}
        \put(70,14.5){DeepIM\cite{li2018deepim}}
        \put(86,18){2021}
        \put(86,14.5){SO-Pose\cite{Di_2021_ICCV}}
    \end{overpic}
    \caption{位姿估计发展时间线}
    \label{fig:时间线}
\end{figure}

在传统方法阶段（2017年之前），如\autoref{fig:时间线}蓝色区域所示，研究者主要依赖人工设计的特征描述子，包括SIFT\cite{SIFT}等图像特征，以及FPFH\cite{FPFH}、VFH\cite{VFH}等点云特征。点对特征（PPF）\cite{PPF, PPF1, PPF2, PPF3}通过建立点云局部几何关系进行位姿估计，但这些方法在复杂场景中面临特征稳定性与鲁棒性不足的瓶颈\cite{ycbv, wang2019densefusion}。

随着深度学习技术的快速发展，如图\ref{fig:时间线}橙色区域所示，基于数据驱动的深度学习方法已逐渐成为姿态估计领域的主流范式。PoseCNN \cite{xiang2018posecnn}和DeepIM \cite{li2018deepim}等开创性工作率先验证了深度学习方法在预测精度上的显著优势，而SO-Pose \cite{Di_2021_ICCV}等最新研究则进一步突破了定位精度的上限，推动了该领域研究的不断进步。

\subsection{训练数据范式分类}

依据训练数据模式，现有方法可分为三类（见\autoref{fig:位姿估计任务分类}）：
\begin{itemize}
\item \textbf{实例级方法}：针对特定物体的CAD模型进行训练，具备亚毫米级定位精度与毫秒级推理速度，但泛化能力受限
\item \textbf{类别级方法}：学习同类物体的形状先验，实现类内物体泛化，需大量标注数据支撑
\item \textbf{零样本方法}：通过多物体联合训练提取通用特征，支持零样本位姿估计，但存在精度-泛化权衡问题
\end{itemize}

\begin{figure}[htbp]
    \centering
    \begin{overpic}[width=0.85\textwidth]{figure/intro/位姿估计任务分类.jpg}
        \put(11,69){实例级方法}
        \put(11,44.5){类别级方法}
        \put(11,19){零样本方法}
        \put(83,59){结果}
        \put(41,67){\footnotesize 在该物体训练}
        \put(41,62){\footnotesize 挖掘该物体特征}
        \put(41,57){\footnotesize 精度最高，速度最快}
        \put(41,52){\footnotesize 抗遮挡能力最强}
        
        \put(41,42){\footnotesize 在同类物体训练}
        \put(41,37){\footnotesize 充分挖掘同类物体特征}
        \put(41,32){\footnotesize 同类物体泛化}
        \put(41,27){\footnotesize 方法性能适中}
        
        \put(41,21){\footnotesize 在多种物体训练}
        \put(41,16){\footnotesize 挖掘鲁棒的可匹配特征}
        \put(41,11){\footnotesize 新物体泛化能力}
        \put(41,6){\footnotesize 精度降低、速度慢}
        \put(41,1){\footnotesize 抗遮挡能力有待提升}
    \end{overpic}
    \caption{根据任务对位姿估计方法分类}
    \label{fig:位姿估计任务分类}
\end{figure}

典型实例级方法通过构建物体级特征空间实现精准匹配，而NOCS\cite{NOCS}开创性地提出标准化物体坐标系实现类别级估计。后续研究通过形状先验建模\cite{SGPA, DPDN}、隐式表面表示\cite{GPV-Pose, HS-Pose}等途径提升类内泛化能力。面向开放世界的零样本方法\cite{Gen6D, MegaPose}则突破CAD模型依赖，但需解决跨域特征对齐问题。

\subsection{传感器模态分类}

传感器配置直接影响位姿估计系统的性能边界：
\begin{itemize}
\item \textbf{RGB相机}：成本低、易部署，但缺乏深度信息导致尺度模糊
\item \textbf{RGB-D相机}：主动式深度感知提升估计精度，但受限于材质反射特性（金属/透明物体）与动态范围
\item \textbf{双目相机}：被动深度估计避免主动光源干扰，但基线距离制约深度分辨率
\item \textbf{纯深度相机}：适用于结构光受限场景，但缺失纹理信息影响特征匹配
\end{itemize}

当前研究主要聚焦RGB与RGB-D模态，其中RGB-D方法在YCB-Video\cite{ycbv}等基准数据集上达到$90\%$以上ADD-S精度，而纯RGB方法更适用于消费级应用场景。

\subsection{核心挑战分析}

\autoref{fig:困难与挑战}揭示了位姿估计技术在实际部署中面临的七大挑战：
\begin{itemize}
\item \textbf{杂乱场景}：未在训练集中出现的复杂背景和干扰物
\item \textbf{密集物体堆叠}：引发严重的相互遮挡与错误实例分割
\item \textbf{光照变化}：光线色彩、曝光或阴影导致表观特征失配
\item \textbf{域间差异}：使用仿真数据训练导致与真实场景的材质、模型等差异
\item \textbf{对称物体}：物体本身具有的对称性导致多解问题
\item \textbf{实时性约束}：高精度估计与计算资源的平衡
\item \textbf{非朗伯体材质}：金属/透明物体破坏光度一致性假设
\end{itemize}

该领域的演进脉络映射了学界针对上述核心瓶颈的渐进式突破轨迹。当前主流方法已构建起应对各类典型挑战的完整技术体系，相关研究正从基础解决方案构建转向更精细的优化维度，致力于向更高精度、更强鲁棒性、实时性能等纵深领域探索。

\begin{figure}[htbp]
    \centering
    \begin{overpic}[width=0.85\textwidth]{figure/intro/困难与挑战.jpg}
        \put(2.8,56.5){物体堆叠}
        \put(33.5,57){光照变化}
        \put(85,55.5){杂乱背景}
        \put(61.5,57.5){遮挡}
        \put(3,8.5){金属物体}
        \put(23,2.2){透明物体}
        \put(59.5,2.5){仅仿真数据训练}
    \end{overpic}
    \caption{位姿估计任务面临的困难与挑战}
    \label{fig:困难与挑战}
\end{figure}

\section{研究现状}

本研究聚焦于实例级物体位姿估计算法，重点关注RGB相机和RGB-D相机的算法应用。后续现状分析将专注于实例级位姿估计，按相机类型进行方法分类。

\subsection{基于RGB的物体位姿估计}
\subsubsection{传统方法}
\par 该部分传统方法指的是不使用网络的方法，只使用模式识别、特征提取等方式实现位姿估计的方法。传统方法可以分为迭代匹配（Iterative matching）\cite{lowe1987three}，特征匹配（Feature matching）\cite{rublee2011orb}，模板匹配（Template matching）\cite{hinterstoisser2013model}等方法，如所示。迭代匹配方法通过迭代优化位姿，使相对应位姿渲染的边缘、轮廓等指定的特征与输入图片中的边缘、轮廓贴合。特征匹配方法通过寻找3维模型和2维图像中特征点的对应关系，然后使用perspective-n-point（PnP）算法\cite{lepetit2009ep}计算得到物体的位姿。模板匹配方法将3维模型在各种姿态下渲染得到2维图像，通过查找与输入图像最相似的渲染得到的2维图像，将这张渲染得到的2维图像的位姿作为输入图像的位姿。对于无纹理物体（特征少）、前景遮挡（特征被遮挡）、背景杂乱（特征难提取）、环境光照变化（特征不鲁棒）、3维模型不准（特征无法匹配）等情况，都会导致传统方法结果不佳。
\subsubsection{深度学习方法}
\par 利用深度学习网络强大的拟合能力，将深度学习网络技术结合到位姿估计传统方法上的一些尝试，也取得了不错的效果。我们将基于网络的方法分为以下几类：直接回归法、稀疏特征匹配法、密集特征匹配法、模板匹配法、迭代优化法。
% \texbf{直接回归法}
\par 直接回归法指的是直接从RGB图像作为输入，使用网络的方式对位姿进行分类或者回归的方法，该类方法的典型代表包括SSD-6D\cite{kehl2017ssd}，Deep-6DPose\cite{do2018deep}，PoseCNN\cite{xiang2018posecnn}, EfficientPose\cite{bukschat2020efficientpose}。
% \texbf{稀疏特征匹配法}
\par 稀疏特征匹配法受到图像关键点检测算法的启发，首先利用网络的方式在二维平面上预测选取的关键点，然后利用关键点在平面上的坐标表示（2D）与关键点在三维模型中的坐标表示（3D）建立对应关系，该类方法的典型代表包括BB8\cite{rad2017bb8}，YOLO6D\cite{tekin2018real}，分割关键点法\cite{hu2019segmentation}，DOPE。

% \texbf{密集特征匹配法 } 密集特征匹配法不专门选取关键点，而是预测图像中物体区域的像素对应的三维坐标，利用稠密的2D-3D对应关系，计算物体的位姿。密集特征匹配法由于利用的信息更多，对于遮挡等情况有较好的鲁棒性。该类方法的典型代表包括iPose\cite{hosseini2019ipose}，Pix2Pose\cite{park2019pix2pose}，DPOD\cite{zakharov2019dpod}，CDPN\cite{li2019cdpn}等。
% \texbf{模板匹配法}
\par 模板匹配法延续传统方法中模板匹配的思路，将三维模型在各种姿态下渲染得到二维图像，通过查找与输入图像最相似的渲染得到的三维图像，将这张渲染得到的二维图像的位姿作为输入图像的位姿。该方法的典型代表是AAE\cite{sundermeyer2018implicit}。
% \texbf{迭代优化法}
\par 迭代优化法延续传统方法中迭代匹配的思路，将渲染器得到的初始估计位姿渲染下的图像与输入图像进行对比，得出一个更加优化的估计位姿，重复上述过程最后得到更加精确的估计位姿。该类方法往往用于后处理，即提升其他方法的最终性能。该方法的典型代表包括DeepIM\cite{li2018deepim}，CosyPose\cite{labbe2020cosypose}等。

\subsection{基于RGB-D的物体位姿估计}

\subsubsection{传统方法}

\subsubsection{深度学习方法}

\section{数据集和评价指标}
\par Hinterstoisser等人~\cite{lm}于2013年发布的LM（LineMod）数据集最经典的数据集，其中的物体完全没有遮挡也没有光照变化，物体居于图片中央且易于区分，在位姿估计发展前期起到了重要的作用。LM-O（LineMod-Occlusion）数据集是Branchmann等人~\cite{lmo}在LM数据集的基础上标注了被遮挡物体的位姿，也成为了一个经典的数据集。Hodan等人\cite{tless}发布的T-LESS数据集是一个针对工业场景的数据集，包含多种无纹理对称性工业零件并且物体间存在强烈的遮挡关系。YCB-V（YCB-Video）是Yu Xiang等人~\cite{ycbv}发布的针对家庭场景的数据集，所有物体提供了具有纹理信息的模型，物体间存在遮挡关系，若干物体存在对称性。这四个数据集是物体位姿估计领域的经典数据集，被广泛应用于物体位姿估计的评估和比较。
% \par LM-O（LineMod-Occlusion）\cite{hinterstoisser2012model}是一个常用的物体位姿估计数据集，包含了15个物体的模型、渲染图像、真实图像、深度图像等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。LM-O数据集中的物体模型是由多个局部模型组成，每个局部模型包含了多个特征点，特征点的位置、法向量、颜色等信息。
\par BOP（Benchmark for 6D Object Pose Estimation）\cite{hodan2018bop}将多个常用数据集整合成统一格式，并提供了统一的评价指标，成为了6D物体位姿估计领域的标准数据集。BOP数据集包括了多个数据集，如T-LESS、LM-O、ITODD、YCB-Video等，每个数据集包含了多个物体的模型、渲染图像、真实图像、深度图像等信息。
\section {研究内容与贡献}
\par 本文分别从RGB相机和RGB-D相机作为输入的两种情况，主要针对遮挡对实例级物体位姿估计的影响作为出发点，提出基于深度学习的物体位姿估计方法。对于RGB数据作为输入的情况，提出一种基于边缘的优化方法，进一步提升结果准确性。针对RGB-D数据作为输入的情况，研究RGB数据和深度信息数据进行更好的特征融合，提高位姿估计的召回率。最后，研究如何提高模型的泛化能力，使得模型在未见过的环境中也能够准确估计物体的位姿。
\section{本文组织架构}
\par 根据研究内容，本文共分为六个章节，各章节的内容安排如下：
\par 第一章绪论。
\par 第二章主要介绍了
\par 第三章主要介绍了
\par 第四章主要介绍了
\par 第五章主要介绍了
\par 第六章对本文的研究内容进行了总结，并对未来的研究方向进行了展望。


% \par 6自由度物体位姿估计问题是指通过传感器数据计算物体坐标系相对于相机坐标系的位姿变换。位姿变换包括平移分量和旋转分量，能用一个 $4\times 4$ 的位姿变换矩阵描述。传感器数据通常包括单目RGB相机、深度相机或者多目RGB相机或者多个视角的相机组合。物体坐标系和相机坐标系分别定义在物体的模型和相机上。物体的模型通常是一个三维模型，可以是点云、网格（mesh）模型或者CAD模型，通常具有纹理或颜色信息。相机的模型通常是一个消除了畸变的针孔相机模型，相机内参 $K$ 已知。

% \par 刚性物体在空间中的位置和姿态具有六个自由度（6 Degree of Freedom，又称6DoF或6D），其中三个平移自由度，用笛卡尔坐标系中的坐标表示；三个旋转自由度，可以用欧拉角、四元数、旋转矩阵等方式进行表示。物体的6D位姿估计就是通过传感器获取场景图像、点云等信息，最终获得场景中目标物体在相机坐标系或者世界坐标系下的位置和姿态。

% \par 目标物体的位姿估计问题是一个视觉感知问题，是机器人应用中重要的基础问题。只有机器人系统实现目标物体的定位，才能够实现环境交互，例如机械臂实现物体抓取、物体操作以及避障和跟踪等下游任务。目前，操作机器人还无法广泛应用于非工业场景，主要原因便是对于环境理解的能力不足，无法快速适应非结构化甚至未知环境。因此，解决位姿估计问题能够提高操作机器人的环境感知能力，提高操作机器人在非结构化环境中的快速部署能力。

% \par 目前机器人操作领域的研究百花齐放。OpenVLA~\cite{openvla}和RDT~\cite{liu2024rdt}等工作基于视觉-文本-机器人行为的多模态数据进行训练，实现了通过人类指令输入，机器人通过观测图像能够直接输出对应的动作。这类技术有极大的发展前景，但目前仍然受限于数据规模和数据质量，成功率有待提升，并且无法完成精细化的抓取操作。Unidexgrasp++~\cite{wan2023unidexgrasp++}和Yuyang Li等~\cite{li2024grasp} 采用强化学习路线学习机器人抓取动作，目前这类方法以成功抓取物体为目标，能够在仿真环境训练并在真实环境中部署，但无法准确控制目标物体的位姿。基于深度学习的位姿估计方法~\cite{hodan2024bop}利用大量的数据进行训练，能够在真实环境中准确估计目标物体的位姿，并且搭配轨迹规划算法，实现精细化的抓取操作。
% \subsection{机器人抓取领域的应用}
% \subsection{其他领域的应用}
% \subsubsection{一个 Subsubsection}