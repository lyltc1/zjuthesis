\section{实验}

\subsection{数据集}
LM-O数据集\cite{lmo}引入了具有不同遮挡程度的挑战性测试场景，包含各种颜色、形状和大小的无纹理家用物品。这种复杂性因不同程度的遮挡而加剧。该数据集包含1214张图像，仅用于测试，通过标注八个物体在部分遮挡下的姿态，显著提高了难度。这一增加极大地增加了姿态估计任务的复杂性。由于LM-O数据集中的训练图像数量有限，创建了大量合成图像用于训练。姿态估计方法的性能受到合成图像和真实图像之间域差距的极大影响，突显了高级域随机化和适应策略的重要性。借助公开可用的基于物理渲染（PBR）训练图像~\cite{hodan2024bop}，我们利用这些资源来弥合域差距，提高了训练方法的有效性。

\subsection{指标}
我们选择ADD(-S)误差指标作为6DoF姿态估计任务中最广泛使用的度量标准。该指标计算使用预测姿态和使用真实姿态投影到相机域中的模型点之间的平均距离。对于对称物体，该指标识别使用真实姿态投影的最近模型点，而不是匹配相同的模型点。在本文的所有实验中，如果ADD(-S)误差小于物体直径的$10\%$，则预测的姿态被认为是准确的，这是最常接受的阈值。

此外，我们使用BOP挑战赛定义的BOP评分指标~\cite{hodan2024bop}。该评估采用三个指标：可见表面差异（VSD）、最大对称感知表面距离（MSSD）和最大对称感知投影距离（MSPD）。这三个指标的平均值，称为AR，作为整体性能指标。

\subsection{与现有技术的比较}
\input{table/bcfn/tab_lmo_add}
我们将我们的结果与其他仅使用合成数据进行训练的方法进行比较，重点关注LM-O数据集~\cite{brachmann2016lmo}。我们报告ADD(-S)的召回率百分比，并与LM-O数据集上的最新技术进行基准测试，如\autoref{tab:lmo_adds}所述。术语“RGB”表示输入中未使用深度信息，而“RGB-D”表示包含深度信息。我们使用BOP挑战赛2023~\cite{hodan2024bop}提供的默认检测。我们的结果在平均性能方面接近最新方法HiPose，并在ape和glue物体上表现出优越的结果。

我们还报告了BOP评分的结果，如\autoref{tab:lmo_bop}所示。我们的方法在准确性上与SurfEmb~\cite{haugaard2022surfemb}相当，但我们的执行速度远远超过SurfEmb。向上箭头（↑）表示较高的指标值表示更好的性能，而向下箭头（↓）表示相反。术语“PBR”表示模型仅在合成图像上训练。

\input{table/bcfn/tab_lmo_bop}
\subsection{消融研究}
\input{table/bcfn/tab_output_size}
我们研究了各种超参数对ADD(-S)结果的影响，如\autoref{tab:main_ablation}所述。

\textbf{CNN头尺寸的影响 }输入图像被裁剪成边长为256像素的正方形。通过修改上采样层的数量，我们评估了CNN头输出配置在$128 \times 128$和$256 \times 256$分辨率下的效果。较大的CNN头尺寸导致了$0.5\%$的改进。

\textbf{跳跃连接 }基线网络在CNN分支中缺少跳跃连接。我们尝试在具有相同维度的编码器和解码器层之间实现跳跃连接，具体方法是将编码器的输出与某些解码器层连接。引入跳跃连接后，性能提升了$0.26\%$。

\input{table/bcfn/tab_consistent_loss}

\textbf{一致性损失 }我们的标准训练批次大小设置为32。如~\cref{tab:consistent_loss}所示，我们报告了在批次大小为16和24的情况下，应用和不应用一致性损失的结果。结果证实了引入一致性损失的一致有效性。

\textbf{骨干网络 }我们将ResNet-34骨干网络与ConvNeXt骨干网络进行了比较，观察到$0.73\%$的改进。

\subsection{可视化}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\textwidth]{figure/bcfn/visualization.jpg}}
    \caption{可视化结果}
    \label{fig:visualization}
\end{figure}

我们在\autoref{fig:visualization}中展示了我们的定性结果。第一和第三行显示了输入图像，而第二和第四行展示了根据估计姿态渲染在调暗图像上的目标模型。

在\autoref{fig:vis_middle}中，我们展示了一个网络工作示例。最左边的图像是输入图像。在中间的点云图像中，红色和蓝色点云共同构成输入点云。我们的BCFN网络区分点云上的每个点是否属于物体。在这个示例中，红点表示物体上的点，而蓝点表示背景。对于每个红点，网络输出一个二进制编码，该编码可以进一步对应于物体坐标系下的坐标，表示为绿色点云。利用黑线指示的对应关系，我们使用Kabsch算法确定目标物体的姿态，如右图所示。

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.80\textwidth]{figure/bcfn/vis_middle.jpg}}
    \caption{网络工作示例}
    \label{fig:vis_middle}
\end{figure}
