\chapter{基于双向一致性信息融合网络的RGB-D物体位姿估计方法}

本章介绍一种从单幅 RGB-D 图像中进行 6Dof 物体位姿估计的实例级物体位姿估计方法。我们提出了双向一致性融合网络，侧重充分融合RGB图像的外观信息和深度图像的几何信息。上一章节的研究主要利用融合后的深度数据来指导学习，忽略了RGB分支。本章节的方法核心洞察是使用RGB和深度两个分支来监督网络。我们开发了一个双分支网络，将外观和几何信息相结合。随后，我们为每个分支提出了像素级和顶点级损失。此外，我们引入了一致性损失来增强网络的收敛性。我们通过在LM-O数据集上进行的实验展示了我们方法的优越性。

\section{引言}

估计物体的位姿~\cite{CAC1,CAC2}（六自由度，6DoF）是机器人领域的一项基本挑战。准确的位姿估计使得机器人能够规划获取或操作目标物体的轨迹。例如，移动盛满水的杯子或操作钻机等任务需要持续监测物体的位姿。

主要的位姿估计方法利用来自RGB相机~\cite{su2022zebrapose}或RGB-D相机~\cite{2024hipose}的输入，其中RGB-D相机在准确性方面显著超过仅RGB的技术，如在BOP Challenge 2023中所示~\cite{hodan2024bop}。因此，采用RGB-D技术已成为主流方法。

RGB图像和深度图像包含截然不同类型的信息。RGB图像主要捕捉外观细节，而深度图像可以转化为点云，提供关于物体的空间和结构信息。寻找有效的方法来融合RGB和深度图像中的信息仍然是一个持续的努力~\cite{he2021ffb6d}。

FFB6D~\cite{he2021ffb6d}通过映射和合并RGB图像的特征与点云中的特征，利用二维RGB图像像素与三维点云之间的对应关系，从而增强了这些模态之间的信息流。
在FFB6D网络架构的基础上，HiPose~\cite{2024hipose}引入了一种迭代对应修剪方法，以消除不准确的对应关系，从而提高网络的精度。
然而，这两种方法仅在信息层面上融合了RGB和深度的特征，忽视了在监督信号层面上完整利用这两种模态的数据，因为它们仅在融合的点云分支上计算损失。

\begin{figure}[t]
\centering
\begin{overpic}[width=0.68\textwidth]{figure/bcfn/teaser.jpg}
\put (65, 45) {CAD模型}
\put (76.4,7.5) {位姿估计结果}
\put (67.5,30.5) {\rotatebox{270}{Kabsch}}
\end{overpic}
\caption{BCFN的架构}
\label{fig:teaser}
\end{figure}

在本文中，我们介绍了双向一致性融合网络，基于FFB6D~\cite{he2021ffb6d}的基础工作。我们的网络以RGB-D图像为输入，输出逐顶点的对应关系，然后应用Kabsch算法估计位姿，如~\cref{fig:teaser}所示。
与FFB6D不同，我们的方法将RGB单一方法的像素级损失与RGB-D方法的点云级损失相结合。此外，为了维持RGB和深度分支之间的一致性，我们引入了一致性损失，以确保两个分支之间的收敛和一致性。

总之，我们工作的主要贡献如下：
\begin{enumerate}
\item 我们深入探讨了一种用于物体位姿估计的深度多模态数据融合网络，并提出了双向一致性融合网络。
\item 我们通过整合像素级和顶点级损失以及一致性损失，提升了网络的收敛性和准确性。
\item 我们使用LM-O数据集~\cite{lmo}评估了我们的方法，并取得了与最先进技术相当的结果。
\end{enumerate}