\chapter{基于双向一致性信息融合网络的RGB-D物体位姿估计方法}

本章详细介绍了一种从单张 RGB-D 图像中进行 6DoF 物体位姿估计的实例级物体位姿估计方法。我们提出了一种名为双向一致性融合网络（Bidirectional Consistency Fusion Network, BCFN）的新方法，该方法旨在充分融合 RGB 图像的外观信息与深度图像的几何信息，从而实现更高效、更准确的位姿估计。

上一章节的研究主要集中于利用融合后的深度数据来指导网络的学习过程，但在此过程中忽略了对 RGB 分支的充分利用。尽管融合后的深度数据能够在一定程度上包含 RGB 图像分支的点信息，但其在像素层面的连续性信息并未被充分挖掘和利用。这种不足可能会限制网络在处理复杂场景时的表现。

本章节提出的方法的核心洞察在于，通过同时利用 RGB 分支和深度分支对网络进行监督，能够更全面地捕捉和融合多模态数据的特性。为此，我们开发了一个双分支网络架构，该架构能够有效结合 RGB 图像的外观信息与深度图像的几何信息。具体而言，我们为每个分支设计了像素级损失和顶点级损失，以确保网络能够在不同层次上学习到有用的特征。此外，为了进一步增强 RGB 分支与深度分支之间的协同作用，我们引入了一致性损失，旨在促进两个分支之间的收敛性和一致性，从而提升网络的整体性能。

通过在 LM-O 数据集上的实验验证，我们展示了所提出方法的优越性。实验结果表明，双向一致性融合网络能够在准确性和收敛性方面显著优于其他方法，为 6DoF 物体位姿估计任务提供了一种有效的解决方案。

\section{引言}

估计物体的位姿（六自由度，6DoF）是机器人领域的一项基本挑战。准确的位姿估计使得机器人能够规划获取或操作目标物体的轨迹。例如，移动盛满水的杯子或操作钻机等任务需要持续监测物体的位姿。位姿估计的精度和鲁棒性直接影响到机器人在复杂环境中执行任务的能力，尤其是在存在遮挡、光照变化或纹理缺乏的情况下。为了应对这些挑战，研究人员提出了多种方法，试图通过结合不同模态的数据来提高位姿估计的性能。

当前主流的位姿估计方法通常依赖于来自RGB相机\cite{su2022zebrapose}或RGB-D相机\cite{2024hipose}的输入。其中，RGB-D相机由于能够同时捕获外观信息和几何信息，在准确性方面显著优于仅依赖RGB的技术。这一点在BOP Challenge 2023\cite{hodan2024bop}中得到了充分验证，RGB-D技术在多个任务中表现出色。因此，基于RGB-D的位姿估计方法已逐渐成为研究和应用的主流方向。

RGB图像和深度图像包含截然不同类型的信息。RGB图像主要捕捉外观细节，例如颜色、纹理和边缘，而深度图像可以转化为点云，提供关于物体的空间和结构信息，例如形状、距离和几何特征。寻找有效的方法来融合RGB和深度图像中的信息仍然是一个持续的努力\cite{he2021ffb6d}。

现有的研究表明，单独使用RGB图像或深度图像通常无法充分捕捉复杂场景中的所有信息。例如，RGB图像在光照变化或纹理缺乏的情况下可能表现不佳，而深度图像在处理透明或反射表面时可能会出现噪声或数据丢失。因此，结合这两种模态的信息可以弥补各自的不足，从而提高位姿估计的鲁棒性和准确性。

为了实现这一目标，研究人员提出了多种融合策略，包括特征级融合、决策级融合和监督级融合。其中，特征级融合通过提取和合并RGB和深度图像的特征来增强模态间的信息流，而决策级融合则在网络的输出阶段结合来自不同模态的预测结果。相比之下，监督级融合更进一步，通过在训练过程中对每个模态施加独立的监督信号，确保网络能够充分学习到每种模态的特性。

然而，如何在融合过程中保持RGB和深度分支之间的一致性仍然是一个挑战。模态之间的不一致可能导致信息丢失或冲突，从而影响网络的性能。因此，设计一种能够有效捕捉和融合多模态信息，同时促进模态间一致性的网络架构具有重要意义。

例如，FFB6D\cite{he2021ffb6d}通过映射和融合RGB图像的特征与点云中的特征，利用二维RGB图像像素与三维点云之间的对应关系，增强了模态间的信息流。在此基础上，HiPose\cite{2024hipose}进一步引入了一种迭代对应修剪方法，以消除不准确的对应关系，从而提升了网络的精度。然而，这两种方法仅在特征层面上实现了RGB和深度信息的融合，而在监督信号层面上未能充分利用这两种模态的数据，因为它们的损失计算仅限于融合后的点云分支。

在本文中，我们提出了一种名为双向一致性融合网络（Bidirectional Consistency Fusion Network, BCFN）的新方法，该方法基于FFB6D\cite{he2021ffb6d}的基础工作进行改进。我们的网络以RGB-D图像为输入，输出逐顶点的对应关系，并通过Kabsch算法估计物体的6DoF位姿，如\autoref{fig:bcfn_teaser}所示。

\begin{figure}[t]
    \centering
    \begin{overpic}[width=0.68\textwidth]{figure/bcfn/teaser.jpg}
    \put (65, 45) {CAD模型}
    \put (76.4,7.5) {位姿估计结果}
    \put (67.5,30.5) {\rotatebox{270}{Kabsch}}
    \end{overpic}
    \caption{BCFN的架构}
    \label{fig:bcfn_teaser}
    \end{figure}

与FFB6D不同，我们的方法在融合策略上进行了创新。具体而言，我们结合了RGB分支的像素级损失和RGB-D分支的点云级损失，从而在不同层次上对网络进行监督。这种多层次的监督机制能够更全面地捕捉RGB图像的外观信息和深度图像的几何信息。此外，为了进一步增强RGB分支和深度分支之间的协同作用，我们引入了一致性损失。该损失旨在促进两个分支之间的收敛性和一致性，从而有效缓解模态间可能存在的信息冲突或不一致问题。

通过这种设计，我们的方法不仅能够充分利用RGB和深度图像的互补特性，还能够在融合过程中保持模态间的信息流畅性和一致性。实验结果表明，双向一致性融合网络在准确性和鲁棒性方面显著优于现有方法，为复杂场景下的6DoF物体位姿估计提供了一种高效且可靠的解决方案。

总之，我们的主要贡献如下：
\begin{enumerate}
\item 提出了一种用于物体位姿估计的深度多模态数据融合网络——双向一致性融合网络（BCFN）。
\item 通过结合像素级损失、顶点级损失以及一致性损失，显著提升了网络的收敛性和预测准确性。
\item 在LM-O数据集上对所提方法进行了全面评估，实验结果表明其性能优于现有的最先进技术。
\end{enumerate}

\inputbody{3_2_bcfn_method.tex}
\inputbody{3_3_bcfn_experiment.tex}

\section{本章小结}
本章提出了一种基于双向一致性信息融合网络（BCFN）的RGB-D物体位姿估计方法。该方法通过双分支协同学习机制，充分融合了RGB图像的外观信息与深度图像的几何信息。具体而言，设计了像素级损失和顶点级损失以优化RGB分支和深度分支的特征提取能力，并引入跨模态一致性损失以增强模态间的协同作用。实验结果表明，所提出的BCFN在准确性和鲁棒性方面显著优于现有方法，为6DoF物体位姿估计任务提供了一种高效且可靠的解决方案。