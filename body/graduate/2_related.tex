\chapter{国内外研究现状}
\par 6自由度物体位姿估计问题是指通过传感器数据计算物体坐标系相对于相机坐标系的位姿变换。位姿变换包括平移分量和旋转分量，能用一个 $4\times 4$ 的位姿变换矩阵描述。传感器数据通常包括单目RGB相机、深度相机或者多目RGB相机或者多个视角的相机组合。物体坐标系和相机坐标系分别定义在物体的模型和相机上。物体的模型通常是一个三维模型，可以是点云、网格（mesh）模型或者CAD模型，通常具有纹理或颜色信息。相机的模型通常是一个消除了畸变的针孔相机模型，相机内参 $K$ 已知。
\par 刚性物体在空间中的位置和姿态具有六个自由度（6 Degree of Freedom，又称6DoF或6D），其中三个平移自由度，用笛卡尔坐标系中的坐标表示；三个旋转自由度，可以用欧拉角、四元数、旋转矩阵等方式进行表示。物体的6D位姿估计就是通过传感器获取场景图像、点云等信息，最终获得场景中目标物体在相机坐标系或者世界坐标系下的位置和姿态。
\par 传感器可以采用RGB彩色相机，RGB-D彩色深度相机或者双目立体相机，不同的传感器配置具有不同的特点。RGB彩色相机成本低，易于部署，但用于物体位姿估计的难度最大；RGB-D彩色深度相机的成本较高，且深度相机在帧率、视场角、深度测量范围的限制，使得深度相机不适用于高反光物体、透明物体或者快速运动的物体，但深度信息能够极大地提高位姿估计的精度，用于物体位姿估计的难度最低，目前大部分移动设备都未配备深度相机；双目立体相机能够用双目获取深度信息，但双目之间的距离限制了深度信息的精度，且目前数据集较少，相关研究也比较少。如何仅使用RGB彩色相机实现高精度的位姿估计，是目前的研究热点。

\par 物体位姿估计主要可以分为两类：实例级（instance-level）物体位姿估计和类别级（object level）物体位姿估计。实例级的物体位姿估计指的是物体的模型已知，即物体模型的长宽高均已知且准确。类别级的物体位姿估计的设定是给定物体的类别信息和一个模板化的模型，需要算法在输出位姿信息的同时输出物体模型的尺寸。举例来说，针对一个特定的杯子，实例级物体位姿估计方法需要给定该杯子的准确模型，而类别级物体位姿估计方法会估计这个杯子的长宽高三个方向的尺寸，由此定义物体的模型，进而进行位姿估计。这两类研究并行进展，总体上来说实例级物体位姿估计更能充分利用每一个模型的细节信息，因此能够得到更加准确的位姿估计，并且机器人操作往往需要定义抓取点，因此物体模型也是必要信息。类别级物体位姿估计有更好的泛化能力。本论文围绕实例级物体位姿估计算法展开研究，故以下相关研究仅围绕实例级物体位姿估计展开，不涉及类别级物体位姿估计的问题。
\section{基于RGB的物体位姿估计}
\subsection{传统方法}
\par 该部分传统方法指的是不使用网络的方法，只使用模式识别、特征提取等方式实现位姿估计的方法。传统方法可以分为迭代匹配(Iterative matching)~\cite{lowe1987three}，特征匹配(Feature matching)~\cite{rublee2011orb}，模板匹配(Template matching)~\cite{hinterstoisser2013model}等方法，如所示。迭代匹配方法通过迭代优化位姿，使相对应位姿渲染的边缘、轮廓等指定的特征与输入图片中的边缘、轮廓贴合。特征匹配方法通过寻找3维模型和2维图像中特征点的对应关系，然后使用perspective-n-point(PnP)算法~\cite{lepetit2009ep}计算得到物体的位姿。模板匹配方法将3维模型在各种姿态下渲染得到2维图像，通过查找与输入图像最相似的渲染得到的2维图像，将这张渲染得到的2维图像的位姿作为输入图像的位姿。对于无纹理物体（特征少）、前景遮挡（特征被遮挡）、背景杂乱（特征难提取）、环境光照变化（特征不鲁棒）、3维模型不准（特征无法匹配）等情况，都会导致传统方法结果不佳。
\subsection{深度学习方法}
\par 利用深度学习网络强大的拟合能力，将深度学习网络技术结合到位姿估计传统方法上的一些尝试，也取得了不错的效果。我们将基于网络的方法分为以下几类：直接回归法、稀疏特征匹配法、密集特征匹配法、模板匹配法、迭代优化法。
\subsection{直接回归法}
\par 直接回归法指的是直接从RGB图像作为输入，使用网络的方式对位姿进行分类或者回归的方法，该类方法的典型代表包括SSD-6D~\cite{kehl2017ssd}，Deep-6DPose~\cite{do2018deep}，PoseCNN~\cite{xiang2017posecnn}, EfficientPose~\cite{bukschat2020efficientpose}。
\subsection{稀疏特征匹配法}
\par 稀疏特征匹配法受到图像关键点检测算法的启发，首先利用网络的方式在二维平面上预测选取的关键点，然后利用关键点在平面上的坐标表示（2D）与关键点在三维模型中的坐标表示（3D）建立对应关系，该类方法的典型代表包括BB8~\cite{rad2017bb8}，YOLO6D~\cite{tekin2018real}，分割关键点法~\cite{hu2019segmentation}，DOPE。
\subsection{密集特征匹配法}
\par 密集特征匹配法不专门选取关键点，而是预测图像中物体区域的像素对应的三维坐标，利用稠密的2D-3D对应关系，计算物体的位姿。密集特征匹配法由于利用的信息更多，对于遮挡等情况有较好的鲁棒性。该类方法的典型代表包括iPose~\cite{hosseini2019ipose}，Pix2Pose~\cite{park2019pix2pose}，DPOD~\cite{zakharov2019dpod}，CDPN~\cite{li2019cdpn}等。
\subsection{模板匹配法}
\par 模板匹配法延续传统方法中模板匹配的思路，将三维模型在各种姿态下渲染得到二维图像，通过查找与输入图像最相似的渲染得到的三维图像，将这张渲染得到的二维图像的位姿作为输入图像的位姿。该方法的典型代表是AAE~\cite{sundermeyer2018implicit}。
\subsection{迭代优化法}
\par 迭代优化法延续传统方法中迭代匹配的思路，将渲染器得到的初始估计位姿渲染下的图像与输入图像进行对比，得出一个更加优化的估计位姿，重复上述过程最后得到更加精确的估计位姿。该类方法往往用于后处理，即提升其他方法的最终性能。该方法的典型代表包括DeepIM~\cite{li2018deepim}，CosyPose~\cite{labbe2020cosypose}等。
\section{基于RGB-D的物体位姿估计}
\subsection{传统方法}
\subsection{深度学习方法}
\section{面向泛化的物体位姿估计}

