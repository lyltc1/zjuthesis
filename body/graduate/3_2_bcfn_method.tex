\section{方法介绍}

给定一张RGB-D图像、已知的相机内参$K$以及一组物体的CAD模型，我们的目标是估计每个物体的姿态。该姿态由旋转矩阵$\mathbf{R} \in SO(3)$和平移向量$\mathbf{t} \in \mathbb{R}^3$定义。

在本节中，我们首先介绍我们的网络，并与之前的方法DenseFusion\cite{wang2019densefusion}和FFB6D\cite{he2021ffb6d}进行比较。随后，我们详细描述了CNN分支中的融合模块的架构。最后，我们描述了损失框架，包括双分支的损失和一致性损失。

% The network's dual branches independently extract features from distinct modalities without intercommunication. These features are then merged in the final Dense Fusion Module.

% Bidirectional fusion modules serve as bridges for information exchange throughout the entire flow of the two branches, with fusion occurring at every encoding and decoding layer. CNN features are concatenated into point cloud features for pose estimation

% CNN features are fused into PCN features, and vice versa, enabling mutual enhancement. Each branch undergoes supervised learning independently, while a consistency loss is applied across both branches to ensure alignment.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \begin{overpic}[width=\textwidth]{figure/bcfn/dense_fusion.jpg}
            \put(20, 47.5){CNN Encoder}
            \put(47, 47.5){CNN Decoder}
            \put(20, 2.5){\shortstack{Point Cloud\\Encoder}}
            \put(47, 2.5){\shortstack{Point Cloud\\Decoder}}
            \put(74, 17.5){\shortstack{Dense\\Fusion}}
            \put(88, 21.5){output}
        \end{overpic}
        \caption{DenseFusion 网络结构}
        \label{fig:dense_fusion}
    \end{subfigure}
    \hfill % 添加适当的空格
    \begin{subfigure}[b]{0.75\textwidth}
        \begin{overpic}[width=\textwidth]{figure/bcfn/ffb6d.jpg}
            \put(20, 47.5){CNN Encoder}
            \put(47, 47.5){CNN Decoder}
            \put(20, 2.5){\shortstack{Point Cloud\\Encoder}}
            \put(47, 2.5){\shortstack{Point Cloud\\Decoder}}
            \put(74, 21.5){Fusion}
            \put(88, 21.5){output}
            \put(81, 42){\shortstack{Fusion\\Module}}
        \end{overpic}
        \caption{FFB6D 网络架构}
        \label{fig:ffb6d}
    \end{subfigure}
    \hfill % 添加适当的空格
    \begin{subfigure}[b]{0.75\textwidth}
        \begin{overpic}[width=\textwidth]{figure/bcfn/bcfn.jpg}
            \put(20, 47.5){CNN Encoder}
            \put(47, 47.5){CNN Decoder}
            \put(20, 2.5){\shortstack{Point Cloud\\Encoder}}
            \put(47, 2.5){\shortstack{Point Cloud\\Decoder}}
            \put(88, 13.5){output}
            \put(74, 13.5){Fusion}
            \put(74, 30){Fusion}
            \put(81, 44){\shortstack{Consistent\\Loss}}
        \end{overpic}
        \caption{双向一致性信息融合网络架构}
        \label{fig:bcfn}
    \end{subfigure}
    \caption{网络结构对比} % 总标题
    \label{fig:comparison}
\end{figure}

\subsection{BCFN方法概览}

融合来自不同模态的信息在RGB-D物体姿态估计中仍然是一个长期存在的挑战。在\autoref{fig:comparison}中，我们将我们的网络与DenseFusion~\cite{wang2019densefusion}和FFB6D~\cite{he2021ffb6d}进行比较，这些网络都基于编码器-解码器架构。网络的一个分支处理RGB图像，称为CNN分支，而另一个分支处理点云，称为PCN分支。解码器输出的特征分别称为CNN特征和PCN特征。CNN特征表示为每像素特征，而PCN特征表示为每顶点特征。

DenseFusion~\cite{wang2019densefusion} 引入了一种超越简单拼接的融合策略，采用了如\autoref{fig:dense_fusion}所示的密集融合模块，从而提升了性能。然而，当CNN和PCN分支分离时，无论是特征拼接还是DenseFusion在处理外观相似或表面反光的物体时，性能都会下降。这些情况对单独的CNN或PCN特征提取过程构成了挑战。FFB6D网络在每个编码和解码层进行融合，以从RGB-D图像中学习表示，如\autoref{fig:ffb6d}所示。这里的一个关键见解是，RGB的外观信息和点云的几何信息在特征提取过程中可以作为互补输入。然而，在最后阶段，FFB6D将CNN特征融合到PCN特征中，最终的表示形式来自PCN特征。我们方法的见解在于认识到将CNN特征融合到PCN特征与反之的区别。前者保留了结构信息，使相邻顶点之间的过渡更加平滑，而后者确保了像素之间的过渡。这意味着如果两个顶点在3D空间中很接近，它们更可能通过PCN分支学习受益，而在图像空间中接近的两个像素可以更容易地从CNN分支中获得相似的特征。

\subsection{融合模块}
在我们的网络中，我们为两个分支分别设计了两个头。PCN头模仿了FFB6D的融合模块，其中每个顶点的特征与最近的CNN特征拼接在一起（如果该顶点投影到图像上）。同样，新设计的CNN头也基于相同的原理：每个像素特征与最近的PCN特征拼接在一起（如果顶点投影到图像上）。CNN头由五个卷积层组成，每个卷积层后面跟着批归一化和ReLU激活函数。

\subsection{一致性损失}

ZebraPose~\cite{su2022zebrapose} 和 HiPose~\cite{2024hipose} 使用二进制编码作为中间变量来代替直接学习姿态，并在RGB和RGB-D设置中显示出很好的效果。为了监督，我们对CNN分支采用与ZebraPose相同的策略，使用掩码和二进制编码，并采用HiPose的方法对PCN分支中的每个顶点进行监督。对两个分支都应用L1损失。具体来说，我们生成一个掩码来表示顶点或像素是否是物体的一部分，以及一个表示物体框架内坐标的编码。

然而，分别训练两个分支可能会导致它们之间的输出不一致。为了保持一致性，我们对它们的输出施加了一致性损失。对于匹配的顶点和像素，掩码和二进制编码的输出应该是相同的。

\subsection{实现细节}

我们的BCFN网络有两个分支，分别处理图像和点云。我们向网络输入1）放大的兴趣区域（RoI）图像，以及2）通过从RoI深度图中均匀采样固定数量的像素生成的点云。PCN头包括一个可见性掩码和一个长度为$d$的二进制编码，每个随机选择的点都有这些元素，而CNN头包含相同的元素，但以像素格式表示。我们对可见性掩码和二进制编码都应用L1损失。我们的训练损失定义如下：
\begin{equation}
Loss = L_\text{mask}^{PCN} + \alpha L_\text{code}^{PCN} + L_\text{mask}^{CNN} + \alpha L_\text{code}^{CNN} + L_{\text{consistent}}
\end{equation}
其中，$\alpha$ 是一个权重因子，在所有实验中设置为 $\alpha = 3$，而 $d$ 默认设置为 $2730$。

对于网络骨干，我们采用了最近的ConvNext架构，如ConvNext~\cite{Liu2022ACF}所述，该架构基于ResNet~\cite{He2015DeepRL}。ConvNext展示了与Vision Transformer~\cite{Dosovitskiy2020AnII}相当的性能，同时保持了ResNet的效率和简洁性。

我们使用Adam优化器~\cite{Kingma2014AdamAM}，以固定的学习率$1e-4$，批量大小为32，训练我们的网络$380K$次迭代。在训练过程中，我们对RGB图像应用了ZebraPose~\cite{su2022zebrapose}中的增强方法，并对深度图应用了MegaPose~\cite{Labbe2022MegaPose6P}中的深度增强方法。我们的训练数据集由BOP挑战赛~\cite{hodan2024bop}提供，仅包含合成图像。我们旨在证明我们的方法可以有效地弥合模拟数据和真实数据之间的域间差距。

在推理阶段，我们仅使用PCN分支的输出。前景顶点的二进制编码通过查找表确定在物体坐标系中的对应坐标。随后，使用RANSAC-Kabsch求解器计算最终的姿态。