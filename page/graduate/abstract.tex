\cleardoublepage
\chapternonum{摘要}
\par 物体位姿估计是指估计目标物体的位置和姿态的一项关键技术，在机器人操作中具有重要的应用价值。本研究聚焦于实例级物体位姿估计领域，旨在改进基于RGB和RGB-D输入的位姿估计方法，以提升其精度和实时性。针对复杂场景中的遮挡和对称性歧义问题，本文提出了一系列高精度且实时的实例级6D物体位姿估计方法，显著提高了算法的精度和运行效率。
\par 本文的主要贡献包括以下几个方面。
\par 首先，在RGB-D作为输入的场景下，提出了层次化表面编码与剪枝方法。通过对层次化二进制表面编码的挖掘，设计了一种层次化二进制表面编码使用策略，结合动态离群点剔除机制，提高了观测点云与物体表面坐标点的密集对应关系匹配的准确性，进而提升位姿估计的精度。在LM-O数据集上，该方法的BOP指标达到79.9\%，推理速度较相同精度基于渲染的方法提升40倍。
\par 其次，为进一步探索了通过损失函数进行跨模态特征融合的方案构建了双向一致性融合网络。该方法采用双分支协同学习架构，通过像素级与顶点级损失函数约束RGB与深度信息的特征对齐，并引入跨模态一致性损失优化网络收敛速度。该方法在LM-O数据集上实现了75.7\%的BOP指标，较现有技术提升1.2\%。
\par 此外，针对对称性歧义问题，设计了一种一对多对应关系的编码方法。并基于这种对称感知表面编码设计了端到端网络直接回归位姿参数。该方法在T-LESS数据集上实现了73.6\%的BOP指标，推理速度较同类方法提升3倍，有效解决了对称的歧义性问题。
\par 最后，在RGB作为输入的场景下，探索了无需渲染的位姿优化方法，提出了基于轮廓对齐的位姿优化方法，通过可见掩码与完整掩码提取感兴趣轮廓，并利用几何约束迭代优化初始位姿。该方法在YCB-V数据集上将BOP指标提升了0.9\%，且无需额外训练优化网络。
\par 实验结果表明，本文提出的方法在基准数据集上均达到国际领先水平，为机器人抓取、增强现实等实际应用提供了重要的理论与技术支撑。
\par \textbf{关键词：}计算机视觉；深度学习；实例级物体位姿估计
\cleardoublepage
\chapternonum{Abstract}
\par Object pose estimation is a key technology for estimating the position and orientation of target objects, with significant application value in robotic manipulation. This study focuses on instance-level object pose estimation, aiming to improve pose estimation methods based on RGB and RGB-D inputs to enhance their accuracy and real-time performance. To address occlusion and symmetry ambiguity in complex scenes, this paper proposes a series of high-precision and real-time instance-level 6D object pose estimation methods, significantly improving the accuracy and efficiency of the algorithms.
\par The main contributions of this paper are as follows.
\par First, in scenarios where RGB-D is used as input, a hierarchical surface encoding and pruning method is proposed. By exploring hierarchical binary surface encoding, a hierarchical binary surface encoding strategy is designed, combined with a dynamic outlier removal mechanism to improve the accuracy of dense correspondence matching between observed point clouds and object surface coordinates, thereby enhancing pose estimation accuracy. On the LM-O dataset, this method achieves a BOP score of 79.9\%, with inference speed 40 times faster than rendering-based methods of similar accuracy.
\par Second, to further explore cross-modal feature fusion through loss functions, a bidirectional consistency fusion network is constructed. This method adopts a dual-branch collaborative learning architecture, aligning RGB and depth features through pixel-level and vertex-level loss constraints, and introduces a cross-modal consistency loss to optimize network convergence speed. This method achieves a BOP score of 75.7\% on the LM-O dataset, improving by 1.2\% over the state-of-the-art.
\par Furthermore, to address the symmetry ambiguity problem, a one-to-many correspondence encoding method is designed. Based on this symmetry-aware surface encoding, an end-to-end network is designed to directly regress pose parameters. This method achieves a BOP score of 73.6\% on the T-LESS dataset, with inference speed 3 times faster than similar methods, effectively resolving symmetry ambiguity issues.
\par Finally, in scenarios where RGB is used as input, a pose optimization method without rendering is explored. A contour alignment-based pose optimization method is proposed, which extracts the region of interest contours through visible and complete masks and iteratively optimizes the initial pose using geometric constraints. This method improves the BOP score by 0.9\% on the YCB-V dataset without requiring additional training of optimization networks.
\par Experimental results demonstrate that the proposed methods achieve state-of-the-art performance on benchmark datasets, providing important theoretical and technical support for practical applications such as robotic grasping and augmented reality.
\par \textbf{Keywords:} Computer Vision; Deep Learning; Instance-Level Object Pose Estimation