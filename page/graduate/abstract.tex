\cleardoublepage
\chapternonum{摘要}
\par 六维物体位姿估计是指估计目标物体的位置和姿态的技术，在机器人感知中具有重要的应用价值。本研究聚焦于实例级物体位姿估计领域，旨在改进基于RGB和RGB-D作为输入的位姿估计方法，以提升其精度和实时性。针对复杂场景中的遮挡和对称性歧义问题，本文提出了一系列高精度且实时的实例级物体位姿估计方法，显著提高了算法的精度和运行效率。
\par 本文的主要贡献包括以下几个方面。
\par 第二章在RGB-D作为输入的场景下，提出了层次化表面编码与剪枝方法。通过对层次化二进制表面编码的挖掘，设计了一种层次化二进制表面编码使用策略，结合动态离群点剔除机制，提高了观测点云与物体表面坐标点的密集对应关系匹配的准确性，进而提升位姿估计的精度。该方法在LM-O数据集上的BOP指标达到79.9\%，推理速度较相同精度基于渲染的方法提升40倍。
\par 第三章构建了双向一致性融合网络，进一步探索了通过损失函数进行跨模态特征融合的方案。该方法采用双分支协同学习架构，通过像素级与顶点级损失函数约束RGB与深度信息的特征对齐，并引入跨模态一致性损失优化网络收敛速度。该方法在LM-O数据集上的BOP指标达到了75.7\%，较现有技术提升1.2\%。
\par 第四章针对对称性歧义问题，设计了一对多对应关系的编码方法，并基于这种对称感知表面编码设计了端到端网络直接回归位姿参数。该方法在T-LESS数据集上实现了73.6\%的BOP指标，推理速度较同类方法提升3倍，有效解决了对称的歧义性问题。
\par 第五章在RGB作为输入的场景下，探索了无需渲染的位姿优化方法，提出了基于轮廓对齐的位姿优化方法，通过可见掩码与完整掩码提取感兴趣轮廓，并利用几何约束迭代优化初始位姿。该方法在YCB-V数据集上将BOP指标提升了0.9\%，且无需额外训练优化网络。
\par 实验结果表明，本文提出的方法在基准数据集上均达到国际领先水平，为机器人抓取、增强现实等实际应用提供了重要的理论与技术支撑。
\par \textbf{关键词：}计算机视觉；深度学习；实例级物体位姿估计
\cleardoublepage
\chapternonum{Abstract}
\par Six-dimensional (6D) object pose estimation refers to the technology of estimating the position and orientation of target objects, which holds significant application value in robotic perception. This research focuses on instance-level object pose estimation, aiming to improve pose estimation methods based on RGB and RGB-D inputs to enhance their accuracy and real-time performance. To address challenges such as occlusion and symmetry ambiguity in complex scenes, this paper proposes a series of high-precision and real-time instance-level object pose estimation methods, significantly improving algorithm accuracy and computational efficiency. 
\par The main contributions of this work include the following aspects. \par In Chapter 2, for RGB-D input scenarios, a hierarchical surface encoding and pruning method is proposed. By exploring hierarchical binary surface encoding, we design a hierarchical binary surface encoding strategy combined with a dynamic outlier removal mechanism. This approach enhances the matching accuracy of dense correspondences between observed point clouds and object surface coordinates, thereby improving pose estimation precision. The method achieves 79.9\% BOP metric on the LM-O dataset, with inference speed being 40 times faster than rendering-based methods with comparable accuracy. 
\par Chapter 3 constructs a bidirectional consistency fusion network, further exploring cross-modal feature fusion through loss functions. This dual-branch collaborative learning framework employs pixel-level and vertex-level loss functions to constrain feature alignment between RGB and depth information, while introducing cross-modal consistency loss to optimize network convergence speed. The method achieves 75.7\% BOP metric on the LM-O dataset, representing a 1.2\% improvement over existing technologies. 
\par Chapter 4 addresses symmetry ambiguity by designing a one-to-many correspondence encoding method. Based on this symmetry-aware surface encoding, an end-to-end network is developed to directly regress pose parameters. The method achieves 73.6\% BOP metric on the T-LESS dataset, with inference speed being 3 times faster than comparable approaches, effectively resolving symmetry ambiguity issues. 
\par Chapter 5 explores a rendering-free pose optimization method for RGB input scenarios, proposing a contour-alignment-based pose optimization approach. This method extracts regions of interest through visible and complete masks, and iteratively optimizes initial poses using geometric constraints. On the YCB-V dataset, this approach improves the BOP metric by 0.9\% without requiring additional optimization network training. 
\par Experimental results demonstrate that the proposed methods achieve state-of-the-art performance on benchmark datasets, providing crucial theoretical and technical support for practical applications such as robotic grasping and augmented reality.
\par \textbf{Keywords:} Computer Vision; Deep Learning; Instance-Level Object Pose Estimation