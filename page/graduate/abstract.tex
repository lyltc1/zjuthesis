\cleardoublepage
\chapternonum{摘要}
\par 六维（6D）物体位姿估计通过确定物体三维位置与旋转姿态，为机器人抓取、装配等任务提供关键位姿信息。本研究聚焦实例级估计领域，旨在改进基于RGB/RGB-D输入的位姿估计方法，突破现有精度与实时性瓶颈。针对复杂场景中由遮挡导致的特征退化及对称性引发的姿态歧义难题，构建了融合多模态感知的创新技术体系。
\par 为了提升在RGB-D作为输入的位姿估计算法的精度和实时性，本文在编码方式和网络架构两个方面进行改进。针对当前对表面编码信息挖掘不足的问题，提出了层次化表面编码与剪枝方法。该方法通过设计层次化二进制表面编码策略，结合动态离群点剔除机制，提高了观测点云与物体表面坐标点的密集对应关系匹配的准确性，进而提升了位姿估计的精度。在LM-O数据集上，该方法的BOP指标达到79.9\%，推理速度较相同精度基于渲染的方法提升40倍。针对RGB与深度信息特征对齐不足的问题，提出了双向一致性融合网络，并通过损失函数实现跨模态特征融合。该方法采用双分支协同学习架构，通过像素级与顶点级损失函数约束特征对齐，并引入跨模态一致性损失优化网络收敛速度。在LM-O数据集上，该方法的BOP指标达到75.7\%。
\par 为了提升在RGB作为输入的位姿估计算法的精度和实时性，本文在对称物体编码方式以及位姿优化两个方面进行改进。针对对称性歧义问题，提出了一对多对应关系的对称感知编码方法，并基于此设计了端到端网络直接回归位姿参数。该方法有效解决了对称性歧义问题，在T-LESS数据集上实现了73.6\%的BOP指标，推理速度较基于求解的方法提升3倍。针对位姿优化依赖渲染而导致耗时的问题，提出了基于轮廓对齐的无渲染位姿优化方法。该方法通过可见掩码与完整掩码提取感兴趣轮廓，并利用几何约束迭代优化初始位姿。在YCB-V数据集上，该方法的BOP指标达到74.8\%，且无需额外训练优化网络。
\par 经系统验证，本文提出的方法在基准数据集上均达到国际领先水平，为机器人抓取、增强现实等实际应用提供了重要的理论与技术支撑。
\par \textbf{关键词：}计算机视觉；深度学习；实例级物体位姿估计
\cleardoublepage
\chapternonum{Abstract}
\par Six-dimensional (6D) object pose estimation, which determines the position and orientation of objects, provides critical spatial information for robotic grasping, assembly, and related applications. This study focuses on instance-level pose estimation, aiming to enhance the accuracy and real-time performance of RGB and RGB-D input-based methods while overcoming existing precision and efficiency limitations. To address challenges such as feature degradation caused by occlusions and pose ambiguity induced by object symmetry in complex scenes, an innovative multimodal perception framework is developed.

\par For improving RGB-D-based pose estimation, enhancements are implemented in encoding strategies and network architectures. A hierarchical surface encoding method with dynamic outlier rejection mechanism is proposed to optimize dense correspondence matching between observed point clouds and object surface coordinates. By designing a hierarchical binary surface encoding strategy combined with real-time outlier pruning, this approach achieves a 79.9\% BOP score on the LM-O dataset while demonstrating 40-fold acceleration in inference speed compared to rendering-based methods with equivalent accuracy. Additionally, a bidirectional consistency fusion network addresses insufficient feature alignment between RGB and depth modalities. This dual-branch collaborative learning framework employs pixel-level and vertex-level loss constraints for feature alignment, supplemented by cross-modal consistency loss to accelerate network convergence, achieving a 75.7\% BOP score on the same LM-O benchmark.

\par For RGB input optimization, advancements target symmetric object encoding and pose refinement. A symmetry-aware encoding method based on one-to-many correspondence relationships effectively resolves pose ambiguity in symmetric objects through an end-to-end network architecture, attaining a 73.6\% BOP score on the T-LESS dataset with threefold inference speed improvement over conventional solver-based approaches. Furthermore, a render-free pose optimization technique eliminates computational bottlenecks associated with rendering dependencies. By leveraging visibility masks and geometric constraints for contour alignment-based iterative refinement, this method achieves a 74.8\% BOP score on the YCB-V dataset without requiring additional optimization network training.

\par Comprehensive experimental validation confirms the proposed framework achieves state-of-the-art performance across multiple benchmark datasets, providing robust theoretical foundations and technical solutions for practical applications in robotic manipulation and augmented reality systems.
\par \textbf{Keywords:} Computer Vision; Deep Learning; Instance-Level Object Pose Estimation
