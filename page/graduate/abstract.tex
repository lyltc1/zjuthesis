\cleardoublepage
\chapternonum{摘要}
\par 六维（6D）物体位姿估计是指估计目标物体的位置和姿态的方法，在机器人抓取、装配、搬运等场景中具有重要的应用价值。本研究聚焦于实例级物体位姿估计领域，旨在改进基于RGB和RGB-D作为输入的位姿估计方法，以提升其精度和实时性。针对复杂场景中的遮挡和对称性歧义问题，本文提出了一系列高精度且实时的实例级物体位姿估计方法，显著提高了算法的精度和运行效率。
\par 本文的主要贡献包括以下几个方面。
\par \textbf{层次化表面编码方法}：针对当前对表面编码信息挖掘不足的问题，提出了层次化表面编码与剪枝方法。该方法通过设计层次化二进制表面编码策略，结合动态离群点剔除机制，提高了观测点云与物体表面坐标点的密集对应关系匹配的准确性，进而提升了位姿估计的精度。在LM-O数据集上，该方法的BOP指标达到79.9\%，推理速度较相同精度基于渲染的方法提升40倍。
\par \textbf{双向一致性信息融合网络}：针对RGB与深度信息特征对齐不足的问题，提出了双向一致性融合网络，并通过损失函数实现跨模态特征融合。该方法采用双分支协同学习架构，通过像素级与顶点级损失函数约束特征对齐，并引入跨模态一致性损失优化网络收敛速度。在LM-O数据集上，该方法的BOP指标达到75.7\%。
\par \textbf{对称感知编码框架}：针对对称性歧义问题，提出了一对多对应关系的对称感知编码方法，并基于此设计了端到端网络直接回归位姿参数。该方法有效解决了对称性歧义问题，在T-LESS数据集上实现了73.6\%的BOP指标，推理速度较基于求解的方法提升3倍。
\par \textbf{无渲染位姿优化方法}：针对位姿优化依赖渲染而导致耗时的问题，提出了基于轮廓对齐的无渲染位姿优化方法。该方法通过可见掩码与完整掩码提取感兴趣轮廓，并利用几何约束迭代优化初始位姿。在YCB-V数据集上，该方法的BOP指标达到74.8\%，且无需额外训练优化网络。
\par 实验结果表明，本文提出的方法在基准数据集上均达到国际领先水平，为机器人抓取、增强现实等实际应用提供了重要的理论与技术支撑。
\par \textbf{关键词：}计算机视觉；深度学习；实例级物体位姿估计
\cleardoublepage
\chapternonum{Abstract}
\par Six-dimensional (6D) object pose estimation refers to the method of estimating the position and orientation of target objects, which holds significant application value in scenarios such as robotic grasping, assembly, and transportation. This research focuses on the field of instance-level object pose estimation, aiming to improve pose estimation methods based on RGB and RGB-D inputs to enhance their accuracy and real-time performance. To address challenges such as occlusion and symmetry ambiguity in complex scenes, this paper proposes a series of high-precision and real-time instance-level object pose estimation methods, significantly improving algorithm accuracy and operational efficiency.
\par The main contributions of this paper include the following aspects.
\par \textbf{Hierarchical Surface Encoding Method}: To address the insufficient exploration of surface encoding information, a hierarchical surface encoding and pruning method is proposed. By designing a hierarchical binary surface encoding strategy combined with a dynamic outlier removal mechanism, this method improves the accuracy of dense correspondence matching between observed point clouds and object surface coordinates, thereby enhancing pose estimation precision. On the LM-O dataset, this method achieves a BOP metric of 79.9\%, with inference speed 40 times faster than rendering-based methods of comparable accuracy.
\par \textbf{Bi-directional Consistent Fusion Network}: To address the insufficient alignment of RGB and depth information features, a bidirectional consistency fusion network is proposed, achieving cross-modal feature fusion through loss functions. This method adopts a dual-branch collaborative learning framework, constraining feature alignment through pixel-level and vertex-level loss functions, and introduces cross-modal consistency loss to optimize network convergence speed. On the LM-O dataset, this method achieves a BOP metric of 75.7\%.
\par \textbf{Symmetry-Aware Encoding Framework}: To address the issue of symmetry ambiguity, a symmetry-aware encoding method with one-to-many correspondence is proposed, and an end-to-end network is designed to directly regress pose parameters. This method effectively resolves symmetry ambiguity issues, achieving a BOP metric of 73.6\% on the T-LESS dataset, with inference speed 3 times faster than computational solving based methods.
\par \textbf{Rendering-Free Pose Optimization Method}: To address the time-consuming nature of pose optimization dependent on rendering, a rendering-free pose optimization method based on contour alignment is proposed. This method extracts regions of interest through visible and complete masks and iteratively optimizes initial poses using geometric constraints. On the YCB-V dataset, this method achieves a BOP metric of 74.8\% without requiring additional training of optimization networks.
\par Experimental results demonstrate that the proposed methods achieve state-of-the-art performance on benchmark datasets, providing important theoretical and technical support for practical applications such as robotic grasping and augmented reality.
\par \textbf{Keywords:} Computer Vision; Deep Learning; Instance-Level Object Pose Estimation
